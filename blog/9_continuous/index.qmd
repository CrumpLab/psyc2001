---
title: "Week 9: Orientation effects and algorithms for a continuous recognition procedure"
description: "Additional literature review and solving a small puzzle"
author: "Matt Crump"
date: 10-28-23
date-modified: last-modified
image: week9.jpg
categories:
  - Psyc 2001
  - Pictures
  - Recognition Memory
execute:
  echo: true
  warning: false
  message: false
---

As a quick recap. In our meeting yesterday we tried a pilot version of a continuous recognition memory procedure for pictures with different levels of noise added.
 
I am still in the process of addressing some of the design issues in that experiment that we discussed, and I expect to be finished with that by early next week. Once I have a finalized version I will email you all a link to try the study one more time, and we will look at the data next week.
 
In addition, the assignment for this week is for you to do a little bit of research yourself, this time on rotation and orientation effects.
 
Many studies have shown that inverted or rotated stimuli may impair memory. Your first task is to read the very short paper (which is attached and in the zotero group library):
 
Scapinello, K. F., & Yarmey, A. D. (1970). The role of familiarity and orientation in immediate and delayed recognition of pictorial stimuli. Psychonomic Science, 21(6), 329–330.
 
Then use google scholar, or some other method (e.g., the references from the above paper), to find another paper investigating how rotation or inversion may impact recognition memory.
 
Make a blog post taking notes about the paper, and describing the extra paper that you found. After we finish the pictures in noise experiment, we'll move on to making an experiment that attempts to replicate rotation effects.

## Advanced

This section is not required, but may be of some interest (e.g., you could decide to work on this problem this week instead of the above). In a continuous recognition procedure items are presented one at time and a participants judges whether the item is new or old. A new item is one that has never been presented before. An old item is a repetition of a previously presented item. 

The experiment that we tried in class was an example of a continuous recognition procedure. One of the issues with this design is controlling the mixture of new and old items in the sequence of trials. In our pilot experiment, there was a noticeable bias in our sequence. Specifically, most of the early trials were new items and most of later trials were old items. 

The advanced problem is to consider our approach to the sequencing of new and old items. Ideally, we should be able to control:

1. The proportion of new and old items (e.g. 50% each).
2. The lag between repetitions. That is how many other items are between first and second presentations.
3. Avoiding long runs of all new or all old

Here is an example approach that solves some of the problem, but not all of them.

Let's say we have 100 pictures, represented here by the numbers 1 to 100

```{r}
all_pictures <- 1:100
```

I will use R to duplicate the set of numbers, and then randomly shuffle the order. This process ensures that each item (1 to 100) is presented twice in a random order. As a result, there would be 100 new trials (the first time you see a particular number), and 100 old trials (the second time you a particular number).

```{r}
trial_sequence <- sample(c(1:100,1:100))
```

However, simply shuffling the values randomly produces some other issues. The code chunk below analyzes the above 200 trial sequence to determine whether a particular trial is a new or old item. If the item is old, the lag between presentations is also computed.

```{r}
df <- data.frame()
for(i in 1:200){
  num_positions <- which(trial_sequence %in% trial_sequence[i])
  if(i == num_positions[1]) {
    type <- "new"
    lag <- NA
  }
  if(i == num_positions[2]) {
    type <- "old"
    lag <- num_positions[2] - num_positions[1]
  }
  t_df <- data.frame(trial = i,
                     item = trial_sequence[i],
                     type = type,
                     lag = lag
                     )
  df <-  rbind(df,t_df)
}
```

This results in a data frame codes properties of the trial sequence. Notice that most of the beginning trials are new items, and most of the ending trials are old items. 

```{r}
knitr::kable(df)
```

We can also look at the distribution of lags for old items. The random shuffling produced a wide range of lags, including very short lags.

```{r}
hist(df$lag)
```

## Next steps

Ideally we want to create a different way of mixing items together that allows:

1. Reduction of bias toward new/old items across the sequence. The beginning shouldn't be mostly new items, and the end shouldn't be mostly old. 

2. More control over the lags between first and second presentation of an item. For example, all items might have a minimum lag of 50 or greater.

I'll be working on solutions to this problem, and feel free to try your own as your work for this week. The solution to this type of problem requires an algorithm for mixing the items, and then some analysis of the sequence to show that it expresses the desired properties.

One of the papers in our zotero group library used a continuous recognition procedure, and their method section describes an approach that involves "test" items, and "filler" items. Here's the reference:

Isola, P., Xiao, J., Torralba, A., & Oliva, A. (2011). What makes an image memorable? Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference On, 145–152.


## Updates

Here's what I came up with. 

### Verbal description of algorithm

This algorithm is designed to create a sequence of items with specific characteristics:

1. Choose a total of "n" items to be presented twice, for example, selecting 200 pictures for double presentation.

2. Define a minimum and maximum lag range, representing the number of items between the first and second presentation (e.g., a lag range of 50 to 100).

3. Calculate the total number of trials as the product of the number of pictures (n) times 2, and then add the maximum lag range (e.g., 200 * 2 + 100 = 500 trials).

4. Randomly distribute the first occurrences of the items within the first 400 trials (considering the maximum lag range) to avoid exceeding the trial limit.

5. Assign the remaining unassigned spots in the sequence to the second occurrences of each item. Iterate through the sequence, and for each first-occurrence item, find open positions within the specified lag range. Randomly choose one and fill it with the second occurrence of the current item. Repeat this process throughout the sequence.

6. Any remaining unassigned spots will be filled with "filler" items. The aim is to balance the beginning and end of the sequence to avoid bias toward new or old items. Here's how it's done:
   - Filler items are introduced, which are additional items.
   - A small window of filler items is selected (e.g., four filler items).
   - The array of filler items is doubled, and the order is shuffled (e.g., 1,2,3,4,1,2,3,4 -> 1,3,2,2,4,3,1,4).
   - Filler items are assigned to the missing values in the trial sequence based on these shuffled indices.
   - This process is repeated across the entire sequence to incorporate new sets of filler items.

In summary, the algorithm is designed to generate a sequence of items, ensuring that first and second occurrences of items are appropriately spaced and filling any gaps with filler items to avoid bias of old vs new items at the beginning and end of the sequence.

### R code version

```{r}

# set total number of items that will be repeated
n_items <- 200
# set range of possible lags
lag_range <- c(50,100)
# calculate total_trials needed
total_trials <- (n_items*2)+lag_range[2]

# spread out the first occurrences of each item across the first 400 positions. This ensures a relatively uniform mixture of new items across the first 400 trials.
# initial_spread has 500 slots, 1s are randomly assigned to 200 positions within the first 400 slots. 0s are assigned everywhere else.
# 1s indicate that an item will receive its first presentation in that slot
initial_spread <- c(sample(c(rep(1,200),rep(0,200))),rep(0,100)) 

# create an empty vector to hold the trial sequence. 
# The trial sequence will also have 500 slots, each slot will be filled with a integer representing a specific item (from 1 to 200, and greater than 200 for filler items)
sequence <- c()

# assign first occurrences of each of the 200 items to the locations that had 1s.
sequence[which(initial_spread == 1)] <- 1:200

# get available spots, these are spots with 0s
available <- which(initial_spread == 0)

# create a vector to hold items that have already been presented twice
old <- c()

# loop across each item in sequence
for(i in 1:length(sequence)){
  # make sure item is an integer
  if(is.na(sequence[i]) == FALSE){ 
    # make sure item hasn't been presented twice already
    if(sequence[i] %in% old == FALSE){ 
      # whatever item this is will be presented twice, add it to old
      old <- c(old,sequence[i])
      # get possible open spots that are within the lag range
      possible_spots <- available[available > i+lag_range[1] & available < i+lag_range[2]]
      
      # error checking if there are no spots left
      if(length(possible_spots) < 1) {
        print(i)
        break
      }
      
      # pick a random open spot
      spot_choice <- sample(possible_spots)[1]
      
      # assign the current item to the chosen spot
      sequence[spot_choice] <- sequence[i]
      
      # update the available spots
      initial_spread[spot_choice] <- 1
      available <- which(initial_spread == 0)
    }
  }
}

# At this point the sequence should be full of the values 1 to 200
# with each value occurring twice
# and with the lag between repetitions randomly assigned within the lag range
# all other values are NA, indicating no item has been assigned. 
# These items need to be "filled" with non-critical items
# non-critical items will be extra items with much shorter lags
# they are included to balance the proportion of old/new items across the sequence

# add fillers to NA

# get the number of NAs in the sequence
num_NA <- length(sequence[is.na(sequence)])

# make the count even
if(num_NA %% 2 != 0) num_NA <-  num_NA+1

# figure out how many sets will be needed
blocks <- ceiling(num_NA/8)

# randomly generate sets of item indices with short lags
# make sure there is enough to cover all NAs
new_order <- c(replicate(blocks,sample(c(1:4,1:4))))
new_vals <- rep(seq(200,200+((blocks-1)*4),4),each=8)

# Filler items are assigned values greater than 200
NA_sequence <- new_vals+new_order

# add filler items to the sequence
na_cnt <- 0
for(i in 1:length(sequence)){
  if(is.na(sequence[i]) == TRUE){
    na_cnt <- na_cnt + 1
    sequence[i] <- NA_sequence[na_cnt]
  }
}
```

Print out example sequence

```{r}
print(sequence)
```

## To Do

Come back and do some analyses of the sequence to show it has the properties we intend it to have. Note also, this algorithm would need to be implemented in javascript for our experiment (already done).










